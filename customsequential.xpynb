import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

print(tf.config.list_physical_devices('GPU'))

# Modified directory paths for images
dataset_path = '/kaggle/input/blood-cells'

# Image dimensions and batch size configuration
w, h = 199, 199
batch_size = 16

# Paths to the dataset
train_path = os.path.join(dataset_path, 'dataset2-master/dataset2-master/images/TRAIN')
test_path = os.path.join(dataset_path, 'dataset2-master/dataset2-master/images/TEST')
val_path = os.path.join(dataset_path, 'dataset2-master/dataset2-master/images/TEST_SIMPLE')

# Data Augmentation for training dataset
train_img_gen = ImageDataGenerator
    rescale=1.0/255,
    zoom_range=0.1,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode="nearest"
)

# Data generators for train, test, and validation datasets
train_dataset = train_img_gen.flow_from_directory(
    train_path,
    target_size=(w, h),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True
)

test_img_gen = ImageDataGenerator(rescale=1.0/255)
test_dataset = test_img_gen.flow_from_directory(
    test_path,
    target_size=(w, h),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

val_img_gen = ImageDataGenerator(rescale=1.0/255)
val_dataset = val_img_gen.flow_from_directory(
    val_path,
    target_size=(w, h),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

# Extracting class names
classes = list(train_dataset.class_indices.keys())
print("Class names:", classes)

# Building the CNN model
model = models.Sequential([
    layers.Conv2D(64, (5, 5), strides=2, activation='relu', input_shape=(w, h, 3)),
    layers.MaxPooling2D((2, 2), strides=2),
    layers.Conv2D(256, (3, 3), strides=2, activation='relu'),
    layers.MaxPooling2D((3, 3), strides=2),
    layers.Conv2D(128, (3, 3), strides=2, activation='relu'),
    layers.MaxPooling2D((2, 2), strides=1),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dense(len(classes), activation='softmax')  # Adjusted to dynamic number of classes
])

# Model compilation
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model_checkpoint = ModelCheckpoint(
    filepath='best_model.h5',
    save_best_only=True,
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max'
)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7)

# Training the model
tf.random.set_seed(42)
epochs = 30
history = model.fit(
    train_dataset,
    epochs=epochs,
    validation_data=val_dataset,
    callbacks=[early_stopping, model_checkpoint, reduce_lr]
)

# Visualizing training results
plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.subplot(2, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.show()

# Loading the best model
model = models.load_model('best_model.h5')

# Evaluating on test dataset
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

# Predicting labels for the validation dataset
predictions = model.predict(val_dataset)
predicted_labels = [classes[prediction.argmax()] for prediction in predictions]
true_labels = val_dataset.classes

# Computing the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels, labels=classes)

# Visualizing the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
